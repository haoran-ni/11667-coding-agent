{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T01:19:38.143639Z",
     "start_time": "2024-11-27T01:19:28.203694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ],
   "id": "48b306379d2d72a9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T01:19:40.437460Z",
     "start_time": "2024-11-27T01:19:39.753138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ],
   "id": "eaaffab51ef44440",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenizer.model_max_length",
   "id": "26013041f52cb57d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:09:03.300239Z",
     "start_time": "2024-11-24T22:09:00.678004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# retrieve dataset\n",
    "full_dataset = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\")\n",
    "\n",
    "# Split dataset deterministically (e.g., 80% train, 10% validation, 10% test)\n",
    "train_test_split = full_dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "validation_test_split = train_test_split['test'].train_test_split(test_size=0.5, shuffle=True, seed=42)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = validation_test_split['test']  # Use as evaluation/test dataset"
   ],
   "id": "5d246d01e7f5a356",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:44:59.080822Z",
     "start_time": "2024-11-24T22:44:59.075403Z"
    }
   },
   "cell_type": "code",
   "source": "full_dataset['train'][6]",
   "id": "1936bc1f928e67d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Generate a REST API with Python and Flask that allows users to create, read, update, and delete records from a database.',\n",
       " 'input': 'Not applicable',\n",
       " 'output': \"from flask import Flask, request\\nfrom flask_sqlalchemy import SQLAlchemy\\n\\napp = Flask(name)\\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'\\ndb = SQLAlchemy(app)\\n\\nclass Record(db.Model):\\n id = db.Column(db.Integer, primary_key=True)\\n name = db.Column(db.String(120), unique=True)\\n\\ndb.create_all()\\n\\n@app.route('/records', methods=['GET'])\\ndef get_records():\\n records = Record.query.all()\\n return {'records':[record.name for record in records]}\\n\\n@app.route('/records', methods=['POST'])\\ndef create_record():\\n record = Record(name=request.json['name'])\\n db.session.add(record)\\n db.session.commit()\\n return {'id': record.id}\\n\\n@app.route('/records/int:id', methods=['PUT'])\\ndef update_record(id):\\n record = Record.query.get(id)\\n record.name = request.json['name']\\n db.session.commit()\\n return {'message': 'Record updated'}\\n\\n@app.route('/records/int:id', methods=['DELETE'])\\ndef delete_record(id):\\n Record.query.filter_by(id=id).delete()\\n db.session.commit()\\n return {'message': 'Record deleted'}\\n\\nif name == 'main':\\n app.run(debug=True)\",\n",
       " 'prompt': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a REST API with Python and Flask that allows users to create, read, update, and delete records from a database.\\n\\n### Input:\\nNot applicable\\n\\n### Output:\\nfrom flask import Flask, request\\nfrom flask_sqlalchemy import SQLAlchemy\\n\\napp = Flask(name)\\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'\\ndb = SQLAlchemy(app)\\n\\nclass Record(db.Model):\\n id = db.Column(db.Integer, primary_key=True)\\n name = db.Column(db.String(120), unique=True)\\n\\ndb.create_all()\\n\\n@app.route('/records', methods=['GET'])\\ndef get_records():\\n records = Record.query.all()\\n return {'records':[record.name for record in records]}\\n\\n@app.route('/records', methods=['POST'])\\ndef create_record():\\n record = Record(name=request.json['name'])\\n db.session.add(record)\\n db.session.commit()\\n return {'id': record.id}\\n\\n@app.route('/records/int:id', methods=['PUT'])\\ndef update_record(id):\\n record = Record.query.get(id)\\n record.name = request.json['name']\\n db.session.commit()\\n return {'message': 'Record updated'}\\n\\n@app.route('/records/int:id', methods=['DELETE'])\\ndef delete_record(id):\\n Record.query.filter_by(id=id).delete()\\n db.session.commit()\\n return {'message': 'Record deleted'}\\n\\nif name == 'main':\\n app.run(debug=True)\"}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:09:05.919386Z",
     "start_time": "2024-11-24T22:09:05.274770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from templates import format_user_text, format_assistant_text\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "def format_instruct(sample):\n",
    "    user_text = format_user_text(sample['instruction'], sample['input'])\n",
    "    assistant_text = format_assistant_text(sample['output'])\n",
    "    row_json = [{\"role\": \"user\", \"content\": user_text}, {\"role\": \"assistant\", \"content\": assistant_text}]\n",
    "    return tokenizer.apply_chat_template(row_json)\n",
    "\n",
    "# Define a function to calculate token lengths\n",
    "def compute_lengths(example):\n",
    "    prompt = example['prompt']\n",
    "    input = example['input']\n",
    "    output = example['output']\n",
    "\n",
    "    # Tokenize prompt and target\n",
    "    prompt_length = len(tokenizer(prompt)['input_ids'])\n",
    "    input_length = len(tokenizer(input)['input_ids'])\n",
    "    output_length = len(tokenizer(output)['input_ids'])\n",
    "    message_length = len(format_instruct(example))\n",
    "\n",
    "    return {\n",
    "        \"prompt_length\": prompt_length,\n",
    "        \"input_length\": input_length,\n",
    "        \"output_length\": output_length,\n",
    "        \"message_length\": message_length\n",
    "    }"
   ],
   "id": "33107947f8bc9f54",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:18:15.261005Z",
     "start_time": "2024-11-24T22:18:15.257124Z"
    }
   },
   "cell_type": "code",
   "source": "len(full_dataset['train'])",
   "id": "b1a05925ccdd9219",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18612"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:09:41.628294Z",
     "start_time": "2024-11-24T22:09:12.517167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the function to the dataset\n",
    "lengths_dataset = full_dataset['train'].map(compute_lengths)\n",
    "\n",
    "# Convert to a Pandas DataFrame for easier analysis (optional)\n",
    "import pandas as pd\n",
    "\n",
    "lengths_df = pd.DataFrame(lengths_dataset)"
   ],
   "id": "3474044c0d4d9838",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/18612 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef155bf6ba1a4a33b3988988d2b9a183"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:10:06.427878Z",
     "start_time": "2024-11-24T22:10:06.419554Z"
    }
   },
   "cell_type": "code",
   "source": "lengths_df[\"message_length\"].describe()",
   "id": "66f21a47b217ee4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    18612.000000\n",
       "mean       230.152321\n",
       "std        273.205348\n",
       "min         91.000000\n",
       "25%        147.000000\n",
       "50%        179.000000\n",
       "75%        229.000000\n",
       "max       4109.000000\n",
       "Name: message_length, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:10:16.020366Z",
     "start_time": "2024-11-24T22:10:16.011018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Prompt Lengths Stats:\")\n",
    "print(lengths_df[\"message_length\"].describe())\n",
    "lengths_df[\"message_length\"].quantile([0.5, 0.75, 0.9, 0.99])"
   ],
   "id": "98621c5d84ffdc06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Lengths Stats:\n",
      "count    18612.000000\n",
      "mean       230.152321\n",
      "std        273.205348\n",
      "min         91.000000\n",
      "25%        147.000000\n",
      "50%        179.000000\n",
      "75%        229.000000\n",
      "max       4109.000000\n",
      "Name: message_length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.50     179.00\n",
       "0.75     229.00\n",
       "0.90     308.00\n",
       "0.99    1510.89\n",
       "Name: message_length, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T03:47:06.412831Z",
     "start_time": "2024-11-24T03:47:06.197340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram for total lengths\n",
    "plt.hist(lengths_df[\"prompt_length\"], bins=50)\n",
    "plt.xlabel(\"Total Sequence Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Tokenized Sequence Length Distribution\")\n",
    "plt.show()"
   ],
   "id": "678577228983fd8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, \\\n",
    "    Seq2SeqTrainer\n",
    "import evaluate\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Preprocessing function for evaluation\n",
    "def preprocess_function(examples):\n",
    "    prompt = examples['instruction'] + \"\\n\" + examples['input']\n",
    "    target = examples['output']\n",
    "\n",
    "    # Tokenize input and output\n",
    "    model_input = tokenizer(prompt, truncation=True)\n",
    "    labels = tokenizer(text_target=target, truncation=True)[\"input_ids\"]\n",
    "\n",
    "    model_input[\"labels\"] = labels\n",
    "    return model_input\n",
    "\n",
    "\n",
    "# Tokenize the evaluation dataset\n",
    "tokenized_eval = eval_dataset.map(preprocess_function, batched=False, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")"
   ],
   "id": "c8c8dbedf687ca75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T03:20:34.782491Z",
     "start_time": "2024-11-24T03:20:33.322141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "# # Setup evaluation\n",
    "# nltk.download(\"punkt\", quiet=True)\n",
    "# metric = evaluate.load(\"rouge\")\n",
    "# \n",
    "# # Define metric computation function\n",
    "# def compute_metrics(eval_preds):\n",
    "#     predictions, labels = eval_preds\n",
    "#     # Decode predictions and labels\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "# \n",
    "#     # Compute metrics\n",
    "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "# \n",
    "#     # Format the results\n",
    "#     return {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "\n",
    "# Setup evaluation\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return result"
   ],
   "id": "4ff5be394af495fe",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T03:20:46.765805Z",
     "start_time": "2024-11-24T03:20:46.762886Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "d7070acce6b83287",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T03:03:51.453106Z",
     "start_time": "2024-11-24T03:03:51.450538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load generation config from the model\n",
    "generation_config = model.generation_config\n",
    "\n",
    "# Update generation parameters dynamically\n",
    "generation_config.max_length = 4096"
   ],
   "id": "5402b70760de8750",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T03:24:55.432508Z",
     "start_time": "2024-11-24T03:21:11.595331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define training arguments for evaluation-only\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Where checkpoints and metrics will be saved\n",
    "    per_device_eval_batch_size=32,  # Adjust based on available GPU memory\n",
    "    report_to=\"none\",  # Disable external logging\n",
    "    logging_dir=\"./logs\",  # Directory for evaluation logs\n",
    "    fp16=True,  # Use mixed precision if supported by GPU\n",
    "    predict_with_generate=True,  # Enable text generation during evaluation\n",
    "    generation_config=generation_config,  # Configure text generation\n",
    ")\n",
    "\n",
    "# Initialize Trainer for evaluation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_eval.select(range(100)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Perform evaluation\n",
    "print(\"Evaluating the final model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\", eval_results)\n"
   ],
   "id": "e29009837544894",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1270485/312748834.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the final model...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 472.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 383.25 MiB is free. Process 384933 has 5.25 GiB memory in use. Including non-PyTorch memory, this process has 33.34 GiB memory in use. Process 1373880 has 416.00 MiB memory in use. Of the allocated memory 27.64 GiB is allocated by PyTorch, and 5.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 24\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Perform evaluation\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating the final model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 24\u001B[0m eval_results \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Print evaluation results\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluation Results:\u001B[39m\u001B[38;5;124m\"\u001B[39m, eval_results)\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:195\u001B[0m, in \u001B[0;36mSeq2SeqTrainer.evaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mgather\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gen_kwargs \u001B[38;5;241m=\u001B[39m gen_kwargs\n\u001B[0;32m--> 195\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/trainer.py:3975\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   3972\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m   3974\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[0;32m-> 3975\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3976\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3977\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEvaluation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3978\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;49;00m\n\u001B[1;32m   3979\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# self.args.prediction_loss_only\u001B[39;49;00m\n\u001B[1;32m   3980\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   3981\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3982\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3983\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3985\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[1;32m   3986\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_jit_compilation_time\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mmetrics:\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/trainer.py:4169\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   4166\u001B[0m         batch_size \u001B[38;5;241m=\u001B[39m observed_batch_size\n\u001B[1;32m   4168\u001B[0m \u001B[38;5;66;03m# Prediction step\u001B[39;00m\n\u001B[0;32m-> 4169\u001B[0m losses, logits, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprediction_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4170\u001B[0m main_input_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmain_input_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   4171\u001B[0m inputs_decode \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   4172\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_input(inputs[main_input_name]) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minputs\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m args\u001B[38;5;241m.\u001B[39minclude_for_metrics \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   4173\u001B[0m )\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/trainer_seq2seq.py:331\u001B[0m, in \u001B[0;36mSeq2SeqTrainer.prediction_step\u001B[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001B[0m\n\u001B[1;32m    324\u001B[0m summon_full_params_context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    325\u001B[0m     FullyShardedDataParallel\u001B[38;5;241m.\u001B[39msummon_full_params(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel)\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, FullyShardedDataParallel)\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext()\n\u001B[1;32m    328\u001B[0m )\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m summon_full_params_context:\n\u001B[0;32m--> 331\u001B[0m     generated_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgeneration_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgen_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001B[39;00m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39m_from_model_config:\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/generation/utils.py:2215\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2207\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   2208\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   2209\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[1;32m   2210\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   2211\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   2212\u001B[0m     )\n\u001B[1;32m   2214\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[0;32m-> 2215\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2216\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2217\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2218\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2220\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2221\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2222\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2223\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2225\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[1;32m   2226\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[1;32m   2227\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[1;32m   2228\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   2229\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2234\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[1;32m   2235\u001B[0m     )\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/generation/utils.py:3206\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   3203\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[1;32m   3205\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 3206\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   3208\u001B[0m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[1;32m   3209\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_model_kwargs_for_generation(\n\u001B[1;32m   3210\u001B[0m     outputs,\n\u001B[1;32m   3211\u001B[0m     model_kwargs,\n\u001B[1;32m   3212\u001B[0m     is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   3213\u001B[0m )\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/accelerate/utils/operations.py:823\u001B[0m, in \u001B[0;36mconvert_outputs_to_fp32.<locals>.forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/accelerate/utils/operations.py:811\u001B[0m, in \u001B[0;36mConvertOutputsToFp32.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 811\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001B[0m, in \u001B[0;36mautocast_decorator.<locals>.decorate_autocast\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_autocast\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[0;32m---> 44\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/accelerate/utils/operations.py:823\u001B[0m, in \u001B[0;36mconvert_outputs_to_fp32.<locals>.forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/accelerate/utils/operations.py:811\u001B[0m, in \u001B[0;36mConvertOutputsToFp32.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 811\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001B[0m, in \u001B[0;36mautocast_decorator.<locals>.decorate_autocast\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_autocast\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[0;32m---> 44\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 811 (1 times), autocast_decorator.<locals>.decorate_autocast at line 44 (1 times), convert_outputs_to_fp32.<locals>.forward at line 823 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/accelerate/utils/operations.py:823\u001B[0m, in \u001B[0;36mconvert_outputs_to_fp32.<locals>.forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/accelerate/utils/operations.py:811\u001B[0m, in \u001B[0;36mConvertOutputsToFp32.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 811\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001B[0m, in \u001B[0;36mautocast_decorator.<locals>.decorate_autocast\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_autocast\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[0;32m---> 44\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1190\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001B[0m\n\u001B[1;32m   1187\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m   1189\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m-> 1190\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1192\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1193\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1194\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1196\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1197\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1198\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1200\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1203\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpretraining_tp \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:945\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[1;32m    933\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    934\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    935\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    942\u001B[0m         position_embeddings,\n\u001B[1;32m    943\u001B[0m     )\n\u001B[1;32m    944\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 945\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    946\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    947\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    948\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    949\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    950\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    951\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    952\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    953\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    954\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    956\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    958\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:676\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    673\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    675\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[0;32m--> 676\u001B[0m hidden_states, self_attn_weights, present_key_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    680\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    683\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    684\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    687\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    689\u001B[0m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/pscratch/sd/z/zefengc/conda/envs/cmu-llms-final/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:602\u001B[0m, in \u001B[0;36mLlamaSdpaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001B[39;00m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001B[39;00m\n\u001B[1;32m    600\u001B[0m is_causal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m causal_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m q_len \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m--> 602\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalue_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_dropout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    611\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m    612\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mview(bsz, q_len, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 472.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 383.25 MiB is free. Process 384933 has 5.25 GiB memory in use. Including non-PyTorch memory, this process has 33.34 GiB memory in use. Process 1373880 has 416.00 MiB memory in use. Of the allocated memory 27.64 GiB is allocated by PyTorch, and 5.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T07:25:17.901928Z",
     "start_time": "2024-11-24T07:25:17.897982Z"
    }
   },
   "cell_type": "code",
   "source": "model.generation_config",
   "id": "10b7905a616952a5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T04:20:19.796815Z",
     "start_time": "2024-11-24T04:17:39.984590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ],
   "id": "e527dcacc9f2cd51",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Yer lookin' fer me, eh? I be Captain Blackbeak, the most feared pirate to ever sail the seven seas! Me and me crew o' scurvy dogs have been plunderin' and pillagin' fer years, and we'll not rest till we've claimed all the treasure fer ourselves! Me trusty parrot, Polly, be me loyal first mate, and she'll keep me from gettin' too carried away with me love o' gold and booty! Now, what be bringin' ye to these waters?\"}\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T08:10:10.625202Z",
     "start_time": "2024-11-24T08:10:10.621821Z"
    }
   },
   "cell_type": "code",
   "source": "import evaluate",
   "id": "c647ee4b1e63d9b3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T08:10:06.027618Z",
     "start_time": "2024-11-24T08:10:05.936106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluate.list_evaluation_modules(\n",
    "    module_type=\"comparison\",\n",
    "    include_community=False,\n",
    "    with_details=True)"
   ],
   "id": "4acd5db6f703e383",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "evaluate.list_evaluation_modules(\n",
    "    module_type=\"comparison\",\n",
    "    include_community=False,\n",
    "    with_details=True)"
   ],
   "id": "daf3b21470d069ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fb545520665b07d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T06:54:53.199388Z",
     "start_time": "2024-11-24T06:54:44.393149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "1c179f57688ee6f0",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T06:54:53.284787Z",
     "start_time": "2024-11-24T06:54:53.280697Z"
    }
   },
   "cell_type": "code",
   "source": "model.config",
   "id": "2d1a718103b13ac2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 16,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T18:21:10.315624Z",
     "start_time": "2024-11-24T18:21:10.098652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, HfArgumentParser, BitsAndBytesConfig\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_name: str\n",
    "    dataset_name: str\n",
    "    dataset_split: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomBnBConfig:\n",
    "    load_in_4bit: bool\n",
    "    bnb_4bit_quant_type: str\n",
    "    bnb_4bit_compute_dtype: str\n",
    "    bnb_4bit_use_double_quant: bool\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomLoRAConfig:\n",
    "    lora_r: int\n",
    "    # lora_target_modules: str\n",
    "    lora_alpha: int\n",
    "    lora_dropout: float\n",
    "    lora_bias: Literal[\"none\", \"all\", \"lora_only\"]\n",
    "    lora_task_type: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SFTConfig:\n",
    "    sft_max_seq_length: int\n",
    "    sft_packing: bool\n",
    "\n",
    "\n",
    "hfparser = HfArgumentParser((ModelConfig, CustomBnBConfig, CustomLoRAConfig, SFTConfig, TrainingArguments))\n",
    "model_config, custom_bnb_config, custom_lora_config, sft_config, train_args = hfparser.parse_json_file(\n",
    "    json_file=\"../hp_tuning_config_1.json\")  # type: ModelConfig, CustomBnBConfig, CustomLoRAConfig, SFTConfig, TrainingArguments\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=custom_bnb_config.load_in_4bit,\n",
    "    bnb_4bit_quant_type=custom_bnb_config.bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=getattr(torch, custom_bnb_config.bnb_4bit_compute_dtype),\n",
    "    bnb_4bit_use_double_quant=custom_bnb_config.bnb_4bit_use_double_quant\n",
    ")\n"
   ],
   "id": "f36028acfb0a44d3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T21:19:54.541846Z",
     "start_time": "2024-11-24T21:19:54.525905Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.get_device_capability()[0]",
   "id": "98be373a65c307b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T02:45:54.189539Z",
     "start_time": "2024-11-26T02:45:53.646972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Retrieve model\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "id": "a151a92497743b8c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T02:49:21.892254Z",
     "start_time": "2024-11-26T02:49:21.852633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "special_token = \"<|finetune_right_pad_id|>\"\n",
    "\n",
    "print(special_token in tokenizer.get_vocab())\n",
    "\n",
    "tokenizer.convert_tokens_to_ids(special_token)"
   ],
   "id": "1bb5193720c5cbc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128004"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T02:44:42.581653Z",
     "start_time": "2024-11-26T02:44:33.664543Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 9\u001B[0m\n\u001B[1;32m      5\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta-llama/Llama-3.2-1B-Instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id)\n\u001B[0;32m----> 9\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[43mmodel_config\u001B[49m\u001B[38;5;241m.\u001B[39mmodel_name, quantization_config\u001B[38;5;241m=\u001B[39mbnb_config,\n\u001B[1;32m     10\u001B[0m                                              low_cpu_mem_usage\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_config' is not defined"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Retrieve model\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_config.model_name, quantization_config=bnb_config,\n",
    "                                             low_cpu_mem_usage=True)"
   ],
   "id": "c86a9a909d0ba8ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T20:08:48.955863Z",
     "start_time": "2024-11-24T20:08:48.950369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"2222222222\"},\n",
    "    ]\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    truncation=True,\n",
    "    tokenize=False,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(input_ids[0])"
   ],
   "id": "ecb3194975f57e6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 24 Nov 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T18:46:13.920160Z",
     "start_time": "2024-11-24T18:46:13.914625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"1111111111\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"2222222222\"},\n",
    "    ]\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    # add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(input_ids[0])"
   ],
   "id": "40a2185ccba48082",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 24 Nov 2024\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "1111111111<|eot_id|>\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T18:54:22.649184Z",
     "start_time": "2024-11-24T18:54:22.644382Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")",
   "id": "7f0302bb64bdf853",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128009"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T07:56:00.455948Z",
     "start_time": "2024-11-24T07:55:52.429770Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "### Instruction:\n",
      "Use the Task below and the Input given to write the Response, which is a programming code that fulfills the following Task:\n",
      "\n",
      "### Task:\n",
      "Generate a Python code for crawling a website for a specific type of data.\n",
      "\n",
      "### Input:\n",
      "website: www.example.com data to crawl: phone numbers\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "```\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "soup = BeautifulSoup(requests.get('http://www.example.com').text, 'html.parser')\n",
      "for link in soup.find_all('a'):\n",
      "    if link.get('href').startswith('http://www.example.com'):\n",
      "        print(link.get('href'))\n",
      "```\n",
      "### Output:\n",
      "```\n",
      "http://www.example.com/phone\n",
      "http://www.example.com/phone2\n",
      "http://www.example.com/phone3\n",
      "http://www.example.com/phone4\n",
      "http://www.example.com/phone5\n",
      "http://www.example.com/phone6\n",
      "```\n",
      "### Explanation:\n",
      "This code will crawl the website for phone numbers. It will return the phone number to the terminal.\n"
     ]
    }
   ],
   "execution_count": 23,
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n",
    "\n",
    "instruction = \"Generate a Python code for crawling a website for a specific type of data.\"\n",
    "input = \"website: www.example.com data to crawl: phone numbers\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that fulfills the following Task:\n",
    "\n",
    "### Task:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=512, do_sample=True, top_p=0.9, temperature=0.6)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(\n",
    "    f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "\n",
    "\n"
   ],
   "id": "ae72ec6f189b59f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T08:49:25.923296Z",
     "start_time": "2024-11-24T08:49:25.919584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that fulfills the following Task:\n",
    "\n",
    "### Task:\n",
    "1\n",
    "2       \"\"\"\n",
    "\n",
    "print(text)"
   ],
   "id": "3f080bc57b101d72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Use the Task below and the Input given to write the Response, which is a programming code that fulfills the following Task:\n",
      "\n",
      "### Task:\n",
      "1\n",
      "2       \n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11667-final-Z",
   "language": "python",
   "name": "11667-final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
